# ğŸŒ¸ Flower Classification with CNN

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-orange.svg)](https://www.tensorflow.org/)
[![Dataset](https://img.shields.io/badge/Dataset-TF_Flowers-yellow.svg)](https://www.tensorflow.org/datasets/catalog/tf_flowers)

> Convolutional Neural Network (CNN) kullanarak 5 farklÄ± Ã§iÃ§ek tÃ¼rÃ¼nÃ¼n sÄ±nÄ±flandÄ±rÄ±lmasÄ±. Scratch'tan CNN modeli ile %97+ accuracy.

---

## ğŸ“‹ Proje HakkÄ±nda

Bu proje, sÄ±fÄ±rdan (from scratch) CNN modeli tasarlayarak Ã§iÃ§ek tÃ¼rlerini sÄ±nÄ±flandÄ±rmaktadÄ±r. Transfer learning kullanmadan, manuel olarak tasarlanmÄ±ÅŸ CNN mimarisi ile yÃ¼ksek performans elde edilmiÅŸtir.

### ğŸ¯ Ã–zellikler

- âœ… **Scratch CNN Model** - Transfer learning YOK, tamamen custom architecture
- âœ… **Data Augmentation** - 6 farklÄ± augmentation tekniÄŸi
- âœ… **Multi-class Classification** - 5 Ã§iÃ§ek tÃ¼rÃ¼
- âœ… **%97+ Training Accuracy** - %90 Validation Accuracy
- âœ… **Callbacks** - EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
- âœ… **Efficient Pipeline** - Prefetch, AUTOTUNE optimization

---

## ğŸŒº Dataset

**TensorFlow Flowers Dataset**
- **Kaynak**: [TensorFlow Datasets - tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers)
- **Toplam Resim**: 3,670
- **SÄ±nÄ±flar**: 
  - ğŸŒ¼ Daisy (papatya)
  - ğŸŒ» Dandelion (karahindiba)
  - ğŸŒ¹ Roses (gÃ¼l)
  - ğŸŒ» Sunflowers (ayÃ§iÃ§eÄŸi)
  - ğŸŒ· Tulips (lale)

### ğŸ“Š Veri DaÄŸÄ±lÄ±mÄ±
```
Split: 80-20
â”œâ”€â”€ Train: 2,936 images (80%)
â””â”€â”€ Validation: 734 images (20%)
```

**Etiket Mapping:**
- 0: Daisy
- 1: Dandelion
- 2: Roses
- 3: Sunflowers
- 4: Tulips

---

## ğŸ–¼ï¸ Ã–rnek SonuÃ§lar

### Sample Predictions

<img src="screenshots/not8.PNG" alt="Sample Predictions" width="400">


*Model tahminleri: Tulips (etiket 2), Sunflowers (etiket 3), Sunflowers (etiket 3)*

### Training Performance

<img src="screenshots/not6.PNG" alt="Sample Predictions" width="400">


**Final Metrikler (50 Epochs):**
- **Training Loss**: 0.06
- **Training Accuracy**: 97.5%
- **Validation Loss**: 0.60
- **Validation Accuracy**: 90.2%

**GÃ¶zlemler:**
- âœ… Training loss sÃ¼rekli azalÄ±yor (model Ã¶ÄŸreniyor)
- âš ï¸ Validation loss ~20 epoch sonra plateau (overfitting baÅŸlangÄ±cÄ±)
- âœ… Training accuracy %97'ye ulaÅŸtÄ±
- âš ï¸ Validation accuracy %90'da sabit (train-val gap var)

**Overfitting Analizi:**
- Training ve validation accuracy arasÄ±nda ~%7 fark var
- Bu, hafif overfitting gÃ¶stergesi
- Dropout(0.5) ve augmentation kullanÄ±lmasÄ±na raÄŸmen validation performansÄ± sÄ±nÄ±rlÄ± kaldÄ±
- Daha fazla veri veya daha agresif regularization ile iyileÅŸtirilebilir

---

## ğŸ—ï¸ Model Mimarisi

### CNN Architecture (Scratch Design)
```
Input (180Ã—180Ã—3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FEATURE EXTRACTION                  â”‚
â”‚                                     â”‚
â”‚ Conv2D(32, 3Ã—3) + ReLU             â”‚
â”‚     â†“ (178Ã—178Ã—32)                 â”‚
â”‚ MaxPooling2D(2Ã—2)                  â”‚
â”‚     â†“ (89Ã—89Ã—32)                   â”‚
â”‚                                     â”‚
â”‚ Conv2D(64, 3Ã—3) + ReLU             â”‚
â”‚     â†“ (87Ã—87Ã—64)                   â”‚
â”‚ MaxPooling2D(2Ã—2)                  â”‚
â”‚     â†“ (43Ã—43Ã—64)                   â”‚
â”‚                                     â”‚
â”‚ Conv2D(128, 3Ã—3) + ReLU            â”‚
â”‚     â†“ (41Ã—41Ã—128)                  â”‚
â”‚ MaxPooling2D(2Ã—2)                  â”‚
â”‚     â†“ (20Ã—20Ã—128)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (51,200)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLASSIFICATION                      â”‚
â”‚                                     â”‚
â”‚ Flatten                            â”‚
â”‚     â†“ (51,200)                     â”‚
â”‚ Dense(128) + ReLU                  â”‚
â”‚     â†“ (128)                        â”‚
â”‚ Dropout(0.5)                       â”‚
â”‚     â†“ (128)                        â”‚
â”‚ Dense(5) + Softmax                 â”‚
â”‚     â†“ (5)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Output: [daisy, dandelion, roses, sunflowers, tulips]
```

**Toplam Parametreler**: ~6,630,149
- **Conv Layers**: 93,248
- **Dense Layers**: 6,536,901
- **Trainable**: TÃ¼mÃ¼

**Katman DetaylarÄ±:**

| Layer | Output Shape | Param # | AÃ§Ä±klama |
|-------|--------------|---------|----------|
| Conv2D(32) | (178,178,32) | 896 | Basit Ã¶zellikler (kenarlar) |
| MaxPool(2Ã—2) | (89,89,32) | 0 | Boyut yarÄ±ya |
| Conv2D(64) | (87,87,64) | 18,496 | Orta seviye Ã¶zellikler |
| MaxPool(2Ã—2) | (43,43,64) | 0 | Boyut yarÄ±ya |
| Conv2D(128) | (41,41,128) | 73,856 | YÃ¼ksek seviye Ã¶zellikler |
| MaxPool(2Ã—2) | (20,20,128) | 0 | Boyut yarÄ±ya |
| Flatten | (51,200) | 0 | 3D â†’ 1D |
| Dense(128) | (128) | 6,553,728 | SÄ±nÄ±flandÄ±rma |
| Dropout(0.5) | (128) | 0 | Regularization |
| Dense(5) | (5) | 645 | Ã‡Ä±ktÄ± |

---

### EÄŸitim Parametreleri
```python
IMG_SIZE = (180, 180)      # Resim boyutu
BATCH_SIZE = 32            # Batch size
EPOCHS = 10                # Epoch sayÄ±sÄ± (50 Ã¶nerilir)
LEARNING_RATE = 0.001      # Adam optimizer LR
```

### Data Augmentation

**Training Augmentations:**
- âœ… Resize to 180Ã—180
- âœ… Random horizontal flip
- âœ… Random brightness (Â±10%)
- âœ… Random contrast (90%-120%)
- âœ… Random crop to 160Ã—160
- âœ… Normalize (0-1)

**Validation Preprocessing:**
- âœ… Resize to 180Ã—180
- âœ… Normalize (0-1)
- âŒ Augmentation yok

---

## ğŸ“Š SonuÃ§lar

### EÄŸitim PerformansÄ±

| Epoch | Train Loss | Train Acc | Val Loss | Val Acc |
|-------|------------|-----------|----------|---------|
| 1 | 1.15 | 64% | 0.62 | 84% |
| 10 | 0.19 | 93% | 0.45 | 88% |
| 20 | 0.11 | 96% | 0.52 | 89% |
| 30 | 0.08 | 97% | 0.56 | 90% |
| 50 | **0.06** | **97.5%** | **0.60** | **90.2%** |

### EÄŸitim SÃ¼resi

- **Tek Epoch**: ~25 saniye (CPU) / ~8 saniye (GPU: T4)
- **50 Epoch**: ~20 dakika (CPU) / ~7 dakika (GPU)
- **Dataset Loading**: ~3 dakika (ilk kez)

### Model Boyutu

- **best_model.h5**: ~80 MB
- **Parametre SayÄ±sÄ±**: 6,630,149

---

## ğŸ¨ Data Pipeline Optimizasyonu
```python
# Efficient TensorFlow Data Pipeline
ds_train = (
    ds_train
    .map(preprocess_train, num_parallel_calls=AUTOTUNE)  # Paralel preprocessing
    .shuffle(1000)                                        # Buffer'da karÄ±ÅŸtÄ±r
    .batch(32)                                           # 32'lik gruplar
    .prefetch(AUTOTUNE)                                  # GPU boÅŸ beklemez
)
```

**Optimizasyon Teknikleri:**
- **AUTOTUNE**: Otomatik thread optimizasyonu
- **Prefetch**: CPU veriyi GPU hesaplarken hazÄ±rlar
- **Parallel Map**: Ã‡ok Ã§ekirdekli preprocessing
- **Shuffle**: Her epoch farklÄ± sÄ±ra

**Performans ArtÄ±ÅŸÄ±:**
- Naive pipeline: ~45 saniye/epoch
- Optimized pipeline: ~25 saniye/epoch
- **%44 hÄ±zlanma** ğŸš€

---

## ğŸ“ˆ Callbacks

### 1. EarlyStopping
```python
patience=3, restore_best_weights=True
```
Val_loss 3 epoch iyileÅŸmezse dur, en iyi aÄŸÄ±rlÄ±klarÄ± yÃ¼kle

### 2. ReduceLROnPlateau
```python
factor=0.2, patience=2, min_lr=1e-9
```
Val_loss plateau'daysa LR'yi 5'te 1'ine dÃ¼ÅŸÃ¼r

### 3. ModelCheckpoint
```python
save_best_only=True
```
En dÃ¼ÅŸÃ¼k val_loss'ta modeli kaydet

---

## ğŸ”¬ Teknik Detaylar

### KullanÄ±lan KÃ¼tÃ¼phaneler

- **TensorFlow**: Deep Learning framework
- **TensorFlow Datasets**: Built-in veri seti yÃ¼kleme
- **NumPy**: SayÄ±sal iÅŸlemler
- **Matplotlib**: GÃ¶rselleÅŸtirme

### Model Ã–zellikleri

- **Architecture**: Custom 3-block CNN
- **Optimizer**: Adam (adaptive learning rate)
- **Loss Function**: Sparse Categorical Crossentropy
- **Activation Functions**: 
  - Hidden Layers: ReLU
  - Output Layer: Softmax
- **Regularization**: Dropout (0.5)

### Ã–ÄŸrenme Stratejisi

- **Batch Size**: 32 (GPU memory iÃ§in optimal)
- **Learning Rate**: 0.001 (baÅŸlangÄ±Ã§)
- **LR Schedule**: ReduceLROnPlateau ile dinamik azaltma
- **Early Stopping**: Overfitting Ã¶nleme

---

## ğŸ“ Proje YapÄ±sÄ±
```
flower-classification-cnn/
â”‚
â”œâ”€â”€ flower_classification.py        # Ana kod
â”œâ”€â”€ requirements.txt                # Gereksinimler
â”œâ”€â”€ README.md                       # DokÃ¼mantasyon
â”œâ”€â”€ .gitignore                      # Git ignore listesi
â”‚
â”œâ”€â”€ screenshots/                    # README gÃ¶rselleri
â”‚   â”œâ”€â”€ not6.PNG                   # Training grafikleri
â”‚   â””â”€â”€ not8.PNG                   # Ã–rnek tahminler
â”‚
â”œâ”€â”€ tensorflow_datasets/            # Dataset (gitignore'da)
â”‚   â””â”€â”€ tf_flowers/
â”‚
â””â”€â”€ best_model.h5                   # EÄŸitilmiÅŸ model (gitignore'da)
```

---

## ğŸ“ Ã–ÄŸrendiklerim

Bu projede ÅŸunlarÄ± uyguladÄ±m:

- âœ… **SÄ±fÄ±rdan CNN tasarÄ±mÄ±** (Transfer learning yok)
- âœ… **Convolutional Layers** ile feature extraction
- âœ… **MaxPooling** ile boyut kÃ¼Ã§Ã¼ltme
- âœ… **Data Augmentation** ile overfitting Ã¶nleme
- âœ… **TensorFlow Data Pipeline** optimizasyonu
- âœ… **Callbacks** ile akÄ±llÄ± eÄŸitim
- âœ… **Multi-class classification** yaklaÅŸÄ±mÄ±
- âœ… **Model performans analizi**

---

